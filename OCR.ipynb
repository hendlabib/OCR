{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport time\nfrom os import walk\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom skimage.feature import local_binary_pattern\nfrom skimage.measure import find_contours\nfrom skimage.morphology import binary_dilation\nfrom sklearn.svm import SVC\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:41.382082Z","iopub.execute_input":"2022-04-21T22:08:41.382520Z","iopub.status.idle":"2022-04-21T22:08:44.770590Z","shell.execute_reply.started":"2022-04-21T22:08:41.382480Z","shell.execute_reply":"2022-04-21T22:08:44.769101Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Parameters and constants\nAVAILABLE_WRITERS = 672\nRESULTS_FILE = 'results.txt'\nTIME_FILE = 'time.txt'\nOVERLAPPING_METHOD = 0\nLINES_METHOD = 1\nSUPPORT_VECTOR_CLASSIFIER = 0\nNEURAL_NETWORK_CLASSIFIER = 1\nHISTOGRAM_BINS = 256\nNN_LEARNING_RATE = 0.003\nNN_WEIGHT_DECAY = 0.01\nNN_DROPOUT = 0.25\nNN_EPOCHS = 200\nNN_BATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.773359Z","iopub.execute_input":"2022-04-21T22:08:44.773717Z","iopub.status.idle":"2022-04-21T22:08:44.782164Z","shell.execute_reply.started":"2022-04-21T22:08:44.773680Z","shell.execute_reply":"2022-04-21T22:08:44.780859Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def show_images(images, titles=None):\n    n_ims = len(images)\n    if titles is None:\n        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n    fig = plt.figure()\n    n = 1\n    for image, title in zip(images, titles):\n        a = fig.add_subplot(1, n_ims, n)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image)\n        a.set_title(title)\n        n += 1\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.784069Z","iopub.execute_input":"2022-04-21T22:08:44.784771Z","iopub.status.idle":"2022-04-21T22:08:44.797623Z","shell.execute_reply.started":"2022-04-21T22:08:44.784702Z","shell.execute_reply":"2022-04-21T22:08:44.796733Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(img, feature_extraction_method=OVERLAPPING_METHOD):\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        img_copy = img.copy()\n        if len(img.shape) > 2:\n            img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n        img_copy = cv2.medianBlur(img_copy, 5)\n        img_copy = cv2.threshold(img_copy, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n        img_copy = img_copy[min_vertical:max_vertical]\n        return img_copy\n\n    if feature_extraction_method == LINES_METHOD:\n        img_copy = img.copy()\n        if len(img.shape) > 2:\n            grayscale_img = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n        else:\n            grayscale_img = img.copy()\n        img_copy = cv2.threshold(grayscale_img, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n        img_copy = img_copy[min_vertical:max_vertical]\n        grayscale_img = grayscale_img[min_vertical:max_vertical]\n        filter_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n        img_copy_sharpened = cv2.filter2D(img_copy, -1, filter_kernel)\n        return img_copy_sharpened, grayscale_img","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.799324Z","iopub.execute_input":"2022-04-21T22:08:44.799825Z","iopub.status.idle":"2022-04-21T22:08:44.817287Z","shell.execute_reply.started":"2022-04-21T22:08:44.799775Z","shell.execute_reply":"2022-04-21T22:08:44.816090Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_corpus_boundaries(img):\n    crop = []\n    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (100, 1))\n    detect_horizontal = cv2.morphologyEx(img, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n    contours = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = contours[0] if len(contours) == 2 else contours[1]\n    prev = -1\n    for i, c in enumerate(contours):\n        if np.abs(prev - int(c[0][0][1])) > 800 or prev == -1:\n            crop.append(int(c[0][0][1]))\n            prev = int(c[0][0][1])\n    crop.sort()\n    max_vertical = crop[1] - 20\n    min_vertical = crop[0] + 20\n    return min_vertical, max_vertical","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.820070Z","iopub.execute_input":"2022-04-21T22:08:44.820441Z","iopub.status.idle":"2022-04-21T22:08:44.837162Z","shell.execute_reply.started":"2022-04-21T22:08:44.820407Z","shell.execute_reply":"2022-04-21T22:08:44.836045Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def segment_image(img, num, grayscale_img=None):\n    if grayscale_img is not None:\n        grayscale_images = []\n        img_copy = np.copy(img)\n        kernel = np.ones((1, num))\n        img_copy = binary_dilation(img_copy, kernel)\n        bounding_boxes = find_contours(img_copy, 0.8)\n        for box in bounding_boxes:\n            x_min = int(np.min(box[:, 1]))\n            x_max = int(np.max(box[:, 1]))\n            y_min = int(np.min(box[:, 0]))\n            y_max = int(np.max(box[:, 0]))\n            if (y_max - y_min) > 50 and (x_max - x_min) > 50:\n                grayscale_images.append(grayscale_img[y_min:y_max, x_min:x_max])\n        return grayscale_images\n    images = []\n    img_copy = np.copy(img)\n    kernel = np.ones((1, num))\n    img_copy = binary_dilation(img_copy, kernel)\n    bounding_boxes = find_contours(img_copy, 0.8)\n    for box in bounding_boxes:\n        x_min = int(np.min(box[:, 1]))\n        x_max = int(np.max(box[:, 1]))\n        y_min = int(np.min(box[:, 0]))\n        y_max = int(np.max(box[:, 0]))\n        if (y_max - y_min) > 10 and (x_max - x_min) > 10:\n            images.append(img[y_min:y_max, x_min:x_max])\n    return images","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.839226Z","iopub.execute_input":"2022-04-21T22:08:44.839585Z","iopub.status.idle":"2022-04-21T22:08:44.857964Z","shell.execute_reply.started":"2022-04-21T22:08:44.839550Z","shell.execute_reply":"2022-04-21T22:08:44.856630Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def overlap_words(words, avg_height):\n    overlapped_img = np.zeros((3600, 320))\n    index_i = 0\n    index_j = 0\n    max_height = 0\n    for word in words:\n        if word.shape[1] + index_j > overlapped_img.shape[1]:\n            max_height = 0\n            index_j = 0\n            index_i += int(avg_height // 2)\n        if word.shape[1] < overlapped_img.shape[1] and word.shape[0] < overlapped_img.shape[0]:\n            indices = np.copy(overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]])\n            indices = np.maximum(indices, word)\n            overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]] = indices\n            index_j += word.shape[1]\n            if max_height < word.shape[0]:\n                max_height = word.shape[0]\n    overlapped_img = overlapped_img[:index_i + int(avg_height // 2), :]\n    return overlapped_img","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.859758Z","iopub.execute_input":"2022-04-21T22:08:44.860179Z","iopub.status.idle":"2022-04-21T22:08:44.879359Z","shell.execute_reply.started":"2022-04-21T22:08:44.860143Z","shell.execute_reply":"2022-04-21T22:08:44.877417Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_textures(image):\n    index_i = 0\n    index_j = 0\n    texture_size = 100\n    textures = []\n    while index_i + texture_size < image.shape[0]:\n        if index_j + texture_size > image.shape[1]:\n            index_j = 0\n            index_i += texture_size\n        textures.append(np.copy(image[index_i: index_i + texture_size, index_j: index_j + texture_size]))\n        index_j += texture_size\n    return textures","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.880830Z","iopub.execute_input":"2022-04-21T22:08:44.881265Z","iopub.status.idle":"2022-04-21T22:08:44.899189Z","shell.execute_reply.started":"2022-04-21T22:08:44.881219Z","shell.execute_reply":"2022-04-21T22:08:44.898207Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def model_generator(features, labels, feature_extraction_method=OVERLAPPING_METHOD,\n                    classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n    histograms = []\n\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        for texture_array in features:\n            for texture in texture_array:\n                lbp = local_binary_pattern(texture, 8, 3, 'default')\n                histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n                histograms.append(histogram)\n\n    elif feature_extraction_method == LINES_METHOD:\n        for line in features:\n            lbp = local_binary_pattern(line, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            histograms.append(histogram)\n\n    if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n        model = SVC(kernel='linear')\n        model.fit(histograms, labels)\n        return model\n\n    if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n        model = nn.Sequential(nn.Linear(HISTOGRAM_BINS, 128),\n                              nn.ReLU(),\n                              nn.Dropout(p=NN_DROPOUT),\n                              nn.Linear(128, 64),\n                              nn.ReLU(),\n                              nn.Dropout(p=NN_DROPOUT),\n                              nn.Linear(64, 3))\n        model.to(DEVICE)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adamax(model.parameters(), lr=NN_LEARNING_RATE, weight_decay=NN_WEIGHT_DECAY)\n        inputs = torch.Tensor(histograms)\n        labels = torch.tensor(labels, dtype=torch.long) - 1\n        dataset = TensorDataset(inputs, labels)\n        train_loader = torch.utils.data.DataLoader(dataset, batch_size=NN_BATCH_SIZE, shuffle=True)\n        for epoch in range(NN_EPOCHS):\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n                output = model(inputs)\n                loss = criterion(output, labels)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.900481Z","iopub.execute_input":"2022-04-21T22:08:44.900963Z","iopub.status.idle":"2022-04-21T22:08:44.922196Z","shell.execute_reply.started":"2022-04-21T22:08:44.900915Z","shell.execute_reply":"2022-04-21T22:08:44.921104Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_image, feature_extraction_method=OVERLAPPING_METHOD, classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        img = preprocess_image(test_image)\n        words = segment_image(img, 3)\n        avg_height = 0\n        for word in words:\n            avg_height += word.shape[0] / len(words)\n        overlapped_img = overlap_words(words, avg_height)\n        textures = get_textures(overlapped_img)\n        prediction = np.zeros(4)\n        for texture in textures:\n            lbp = local_binary_pattern(texture, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n                prediction[model.predict([histogram])] += 1\n            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n                with torch.no_grad():\n                    model.eval()\n                    histogram = torch.Tensor(histogram)\n                    probabilities = F.softmax(model.forward(histogram), dim=0)\n                    _, top_class = probabilities.topk(1)\n                    prediction[top_class + 1] += 1\n        return np.argmax(prediction)\n\n    if feature_extraction_method == LINES_METHOD:\n        img, grayscale_img = preprocess_image(test_image, feature_extraction_method)\n        grayscale_lines = segment_image(img, 100, grayscale_img)\n        prediction = np.zeros(4)\n        for line in grayscale_lines:\n            lbp = local_binary_pattern(line, 8, 3, 'default')\n            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n                prediction[model.predict([histogram])] += 1\n            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n                with torch.no_grad():\n                    model.eval()\n                    histogram = torch.Tensor(histogram)\n                    probabilities = F.softmax(model.forward(histogram), dim=0)\n                    _, top_class = probabilities.topk(1)\n                    prediction[top_class + 1] += 1\n        return np.argmax(prediction)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.924089Z","iopub.execute_input":"2022-04-21T22:08:44.924747Z","iopub.status.idle":"2022-04-21T22:08:44.947344Z","shell.execute_reply.started":"2022-04-21T22:08:44.924693Z","shell.execute_reply":"2022-04-21T22:08:44.946385Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def read_random_images(root):\n    images = []\n    labels = []\n    test_images = []\n    test_labels = []\n    for i in range(3):\n        found_images = False\n        while not found_images:\n            images_path = root\n            random_writer = random.randrange(AVAILABLE_WRITERS)\n            if random_writer < 10:\n                random_writer = \"00\" + str(random_writer)\n            elif random_writer < 100:\n                random_writer = \"0\" + str(random_writer)\n            images_path = os.path.join(images_path, str(random_writer))\n            if not os.path.isdir(images_path):\n                continue\n            _, _, filenames = next(walk(images_path))\n            if len(filenames) <= 2 and i == 2 and len(test_images) == 0:\n                continue\n            if len(filenames) >= 2:\n                found_images = True\n                chosen_filenames = []\n                for j in range(2):\n                    random_filename = random.choice(filenames)\n                    while random_filename in chosen_filenames:\n                        random_filename = random.choice(filenames)\n                    chosen_filenames.append(random_filename)\n                    images.append(cv2.imread(os.path.join(images_path, random_filename)))\n                    labels.append(i + 1)\n                if len(filenames) >= 3:\n                    random_filename = random.choice(filenames)\n                    while random_filename in chosen_filenames:\n                        random_filename = random.choice(filenames)\n                    chosen_filenames.append(random_filename)\n                    test_images.append(cv2.imread(os.path.join(images_path, random_filename)))\n                    test_labels.append(i + 1)\n    test_choice = random.randint(0, len(test_images) - 1)\n    test_image = test_images[test_choice]\n    test_label = test_labels[test_choice]\n    return images, labels, test_image, test_label","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.948539Z","iopub.execute_input":"2022-04-21T22:08:44.949081Z","iopub.status.idle":"2022-04-21T22:08:44.970104Z","shell.execute_reply.started":"2022-04-21T22:08:44.949044Z","shell.execute_reply":"2022-04-21T22:08:44.968385Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def extract_features(images, labels, feature_extraction_method=OVERLAPPING_METHOD):\n    if feature_extraction_method == LINES_METHOD:\n        lines_labels = []\n        lines = []\n        for image, label in zip(images, labels):\n            image, grayscale_image = preprocess_image(image, feature_extraction_method)\n            grayscale_lines = segment_image(image, 100, grayscale_image)\n            for line in grayscale_lines:\n                lines.append(line)\n                lines_labels.append(label)\n        return lines, lines_labels\n\n    if feature_extraction_method == OVERLAPPING_METHOD:\n        textures = []\n        textures_labels = []\n        for image, label in zip(images, labels):\n            image = preprocess_image(image)\n            words = segment_image(image, 3)\n            avg_height = 0\n            for word in words:\n                avg_height += word.shape[0] / len(words)\n            overlapped_img = overlap_words(words, avg_height)\n            new_textures = get_textures(overlapped_img)\n            textures.append(new_textures)\n            for j in range(len(new_textures)):\n                textures_labels.append(label)\n        return textures, textures_labels","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.972252Z","iopub.execute_input":"2022-04-21T22:08:44.972825Z","iopub.status.idle":"2022-04-21T22:08:44.987639Z","shell.execute_reply.started":"2022-04-21T22:08:44.972777Z","shell.execute_reply":"2022-04-21T22:08:44.986683Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nroot='../input/iam-handwritten-forms-dataset/data'\nfeature_extraction_method=OVERLAPPING_METHOD\nclassifier_type=SUPPORT_VECTOR_CLASSIFIER\ncorrect_predictions = 0\ntotal_execution_time = 0\nfor epoch in range(epochs):\n    images, labels, test_image, test_label = read_random_images(root)\n    start_time = time.time()\n    features, features_labels = extract_features(images, labels, feature_extraction_method)\n    model = model_generator(features, features_labels, feature_extraction_method, classifier_type)\n    prediction = predict(model, test_image, feature_extraction_method, classifier_type)\n    execution_time = time.time() - start_time\n    total_execution_time += execution_time\n    if prediction == test_label:\n        correct_predictions += 1\n    print(\"Epoch #{} | Execution time {} seconds | Model accuracy {}\".format(epoch + 1, round(execution_time, 2), round((correct_predictions / (epoch + 1)) * 100, 2)))\nprint(\"Model accuracy = {}% using {} sample tests.\".format((correct_predictions / epochs) * 100, epochs))\nprint(\"Total execution time = {} using {} sample tests.\".format(round(total_execution_time, 2), epochs))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:08:44.989186Z","iopub.execute_input":"2022-04-21T22:08:44.989554Z","iopub.status.idle":"2022-04-21T22:17:44.560803Z","shell.execute_reply.started":"2022-04-21T22:08:44.989520Z","shell.execute_reply":"2022-04-21T22:17:44.558180Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch #1 | Execution time 5.39 seconds | Model accuracy 100.0\nEpoch #2 | Execution time 4.58 seconds | Model accuracy 100.0\nEpoch #3 | Execution time 4.8 seconds | Model accuracy 100.0\nEpoch #4 | Execution time 4.79 seconds | Model accuracy 100.0\nEpoch #5 | Execution time 4.77 seconds | Model accuracy 100.0\nEpoch #6 | Execution time 3.63 seconds | Model accuracy 100.0\nEpoch #7 | Execution time 3.93 seconds | Model accuracy 100.0\nEpoch #8 | Execution time 4.23 seconds | Model accuracy 100.0\nEpoch #9 | Execution time 4.33 seconds | Model accuracy 100.0\nEpoch #10 | Execution time 4.24 seconds | Model accuracy 100.0\nEpoch #11 | Execution time 3.81 seconds | Model accuracy 100.0\nEpoch #12 | Execution time 4.28 seconds | Model accuracy 100.0\nEpoch #13 | Execution time 4.42 seconds | Model accuracy 100.0\nEpoch #14 | Execution time 4.54 seconds | Model accuracy 100.0\nEpoch #15 | Execution time 4.26 seconds | Model accuracy 100.0\nEpoch #16 | Execution time 5.05 seconds | Model accuracy 100.0\nEpoch #17 | Execution time 4.11 seconds | Model accuracy 94.12\nEpoch #18 | Execution time 4.96 seconds | Model accuracy 94.44\nEpoch #19 | Execution time 4.72 seconds | Model accuracy 94.74\nEpoch #20 | Execution time 4.29 seconds | Model accuracy 95.0\nEpoch #21 | Execution time 4.98 seconds | Model accuracy 95.24\nEpoch #22 | Execution time 4.04 seconds | Model accuracy 95.45\nEpoch #23 | Execution time 4.82 seconds | Model accuracy 95.65\nEpoch #24 | Execution time 5.22 seconds | Model accuracy 95.83\nEpoch #25 | Execution time 4.18 seconds | Model accuracy 96.0\nEpoch #26 | Execution time 4.3 seconds | Model accuracy 96.15\nEpoch #27 | Execution time 4.23 seconds | Model accuracy 96.3\nEpoch #28 | Execution time 4.64 seconds | Model accuracy 96.43\nEpoch #29 | Execution time 5.76 seconds | Model accuracy 96.55\nEpoch #30 | Execution time 4.3 seconds | Model accuracy 96.67\nEpoch #31 | Execution time 3.98 seconds | Model accuracy 96.77\nEpoch #32 | Execution time 3.63 seconds | Model accuracy 96.88\nEpoch #33 | Execution time 4.48 seconds | Model accuracy 96.97\nEpoch #34 | Execution time 4.6 seconds | Model accuracy 97.06\nEpoch #35 | Execution time 4.87 seconds | Model accuracy 97.14\nEpoch #36 | Execution time 3.94 seconds | Model accuracy 97.22\nEpoch #37 | Execution time 4.45 seconds | Model accuracy 97.3\nEpoch #38 | Execution time 4.53 seconds | Model accuracy 97.37\nEpoch #39 | Execution time 4.87 seconds | Model accuracy 97.44\nEpoch #40 | Execution time 4.54 seconds | Model accuracy 97.5\nEpoch #41 | Execution time 4.67 seconds | Model accuracy 97.56\nEpoch #42 | Execution time 3.99 seconds | Model accuracy 97.62\nEpoch #43 | Execution time 4.69 seconds | Model accuracy 97.67\nEpoch #44 | Execution time 4.65 seconds | Model accuracy 97.73\nEpoch #45 | Execution time 4.84 seconds | Model accuracy 97.78\nEpoch #46 | Execution time 4.75 seconds | Model accuracy 97.83\nEpoch #47 | Execution time 4.34 seconds | Model accuracy 97.87\nEpoch #48 | Execution time 3.78 seconds | Model accuracy 97.92\nEpoch #49 | Execution time 4.12 seconds | Model accuracy 97.96\nEpoch #50 | Execution time 4.66 seconds | Model accuracy 98.0\nEpoch #51 | Execution time 5.1 seconds | Model accuracy 98.04\nEpoch #52 | Execution time 4.55 seconds | Model accuracy 98.08\nEpoch #53 | Execution time 4.29 seconds | Model accuracy 98.11\nEpoch #54 | Execution time 4.53 seconds | Model accuracy 98.15\nEpoch #55 | Execution time 4.72 seconds | Model accuracy 98.18\nEpoch #56 | Execution time 4.2 seconds | Model accuracy 98.21\nEpoch #57 | Execution time 3.81 seconds | Model accuracy 98.25\nEpoch #58 | Execution time 3.82 seconds | Model accuracy 98.28\nEpoch #59 | Execution time 4.29 seconds | Model accuracy 98.31\nEpoch #60 | Execution time 3.68 seconds | Model accuracy 98.33\nEpoch #61 | Execution time 3.78 seconds | Model accuracy 98.36\nEpoch #62 | Execution time 4.91 seconds | Model accuracy 98.39\nEpoch #63 | Execution time 5.08 seconds | Model accuracy 98.41\nEpoch #64 | Execution time 4.66 seconds | Model accuracy 98.44\nEpoch #65 | Execution time 4.41 seconds | Model accuracy 98.46\nEpoch #66 | Execution time 4.93 seconds | Model accuracy 98.48\nEpoch #67 | Execution time 5.12 seconds | Model accuracy 98.51\nEpoch #68 | Execution time 5.47 seconds | Model accuracy 98.53\nEpoch #69 | Execution time 5.19 seconds | Model accuracy 98.55\nEpoch #70 | Execution time 5.01 seconds | Model accuracy 98.57\nEpoch #71 | Execution time 4.79 seconds | Model accuracy 98.59\nEpoch #72 | Execution time 4.02 seconds | Model accuracy 98.61\nEpoch #73 | Execution time 4.82 seconds | Model accuracy 98.63\nEpoch #74 | Execution time 6.75 seconds | Model accuracy 98.65\nEpoch #75 | Execution time 4.93 seconds | Model accuracy 98.67\nEpoch #76 | Execution time 4.34 seconds | Model accuracy 98.68\nEpoch #77 | Execution time 4.5 seconds | Model accuracy 98.7\nEpoch #78 | Execution time 4.25 seconds | Model accuracy 98.72\nEpoch #79 | Execution time 4.13 seconds | Model accuracy 98.73\nEpoch #80 | Execution time 4.23 seconds | Model accuracy 98.75\nEpoch #81 | Execution time 4.71 seconds | Model accuracy 98.77\nEpoch #82 | Execution time 4.77 seconds | Model accuracy 98.78\nEpoch #83 | Execution time 4.78 seconds | Model accuracy 98.8\nEpoch #84 | Execution time 4.29 seconds | Model accuracy 98.81\nEpoch #85 | Execution time 4.44 seconds | Model accuracy 98.82\nEpoch #86 | Execution time 3.74 seconds | Model accuracy 98.84\nEpoch #87 | Execution time 4.89 seconds | Model accuracy 98.85\nEpoch #88 | Execution time 4.91 seconds | Model accuracy 98.86\nEpoch #89 | Execution time 4.81 seconds | Model accuracy 98.88\nEpoch #90 | Execution time 5.17 seconds | Model accuracy 98.89\nEpoch #91 | Execution time 4.98 seconds | Model accuracy 98.9\nEpoch #92 | Execution time 4.11 seconds | Model accuracy 98.91\nEpoch #93 | Execution time 4.03 seconds | Model accuracy 98.92\nEpoch #94 | Execution time 5.27 seconds | Model accuracy 98.94\nEpoch #95 | Execution time 4.58 seconds | Model accuracy 98.95\nEpoch #96 | Execution time 5.29 seconds | Model accuracy 98.96\nEpoch #97 | Execution time 4.6 seconds | Model accuracy 98.97\nEpoch #98 | Execution time 4.92 seconds | Model accuracy 98.98\nEpoch #99 | Execution time 4.61 seconds | Model accuracy 98.99\nEpoch #100 | Execution time 4.81 seconds | Model accuracy 99.0\nModel accuracy = 99.0% using 100 sample tests.\nTotal execution time = 456.02 using 100 sample tests.\n","output_type":"stream"}]}]}